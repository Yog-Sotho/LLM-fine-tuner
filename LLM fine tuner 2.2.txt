# --- PATCH: ADD utility normalization functions and refactor sections using them ---

def normalize_columns(df):
    df.columns = [c.strip() for c in df.columns]
    return df

def normalize_dataset_columns(dataset):
    dataset.column_names = [c.strip() for c in dataset.column_names]
    return dataset

def normalize_key(key):
    return key.strip().lower()

def detect_file_type(file) -> str | None:
    name = file.name.lower().strip()
    if name.endswith(".csv"):   return "csv"
    if name.endswith(".jsonl"): return "jsonl"
    if name.endswith(".json"):  return "json"
    if name.endswith(".txt"):   return "txt"
    if name.endswith(".xlsx") and HAS_OPENPYXL: return "excel"
    if name.endswith(".pdf")  and HAS_PDF:      return "pdf"
    return None

def load_dataset_from_file(file, file_type: str, column_mapping: dict | None = None) -> Dataset:
    try:
        if file_type == "jsonl":
            data = []
            with open(file.name, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        data.append(json.loads(line))
            return Dataset.from_list(data)
        
        if file_type == "json":
            with open(file.name, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not isinstance(data, list):
                raise ValueError("JSON file must contain a top-level array of objects.")
            return Dataset.from_list(data)
        
        if file_type == "txt":
            with open(file.name, "r", encoding="utf-8") as f:
                lines = [l.strip() for l in f if l.strip()]
            return Dataset.from_dict({"text": lines})

        if file_type == "pdf":
            text = extract_text_from_pdf(file.name)
            paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
            return Dataset.from_dict({"text": paragraphs})
        
        if file_type == "csv":
            df = pd.read_csv(file.name)
        elif file_type == "excel":
            df = pd.read_excel(file.name, engine="openpyxl")
        else:
            raise ValueError(f"Unsupported file type: {file_type}")

        df = normalize_columns(df)
        if column_mapping:
            column_mapping = {k.strip(): v.strip() for k, v in column_mapping.items()}
            df = df.rename(columns=column_mapping)

        if "instruction" in df.columns and "output" in df.columns:
            return Dataset.from_pandas(df[["instruction", "output"].astype(str)])
        elif "text" in df.columns:
            return Dataset.from_pandas(df[["text"].astype(str)])
        else:
            raise ValueError(
                f"Cannot determine columns automatically. Available: {list(df.columns)}."
                "Please use the column mapping dropdowns above."
            )
    except Exception as e:
        raise RuntimeError(f"Failed to load dataset: {e}")

def validate_and_clean_dataset(dataset: Dataset):
    issues = []
    column_names = [c.strip() for c in dataset.column_names]
    if "text" in column_names:
        lengths = [len(str(t)) for t in dataset["text"]]
    elif "instruction" in column_names and "output" in column_names:
        lengths = [len(str(i)) + len(str(o))
                  for i, o in zip(dataset["instruction"], dataset["output"])]
    else:
        return dataset, ["⚠️ Unknown column structure — cannot validate."]
    
    empty = sum(1 for l in lengths if l == 0)
    if empty:
        issues.append(f"⚠️ {empty} empty examples removed.")
        if "text" in column_names:
            dataset = dataset.filter(lambda x: len(str(x["text"])) > 0)
        else:
            dataset = dataset.filter(
                lambda x: len(str(x["instruction"])) + len(str(x["output"])) > 0
            )
    if len(dataset) == 0:
        issues.append("❌ Dataset is empty after cleaning. No valid examples remain.")
        return dataset, issues
    
    long_ = sum(1 for l in lengths if l > 2048)
    if long_:
        issues.append(f"⚠️ {long_} examples exceed 2048 chars — they will be truncated.")

    return dataset, issues

def get_model_info(model_id: str) -> str:
    m = model_id.lower().strip()
    table = {
        "gpt2-xl":     ("1.5B",   "6 GB"),
        "gpt2-large":  ("774M",   "3 GB"),
        "gpt2-medium": ("355M",   "1.5 GB"),
        "gpt2":        ("124M",   "0.5 GB"),
        "distilgpt2":  ("82M",    "0.3 GB"),
        "opt-125m":    ("125M",   "0.5 GB"),
        "opt-350m":    ("350M",   "1.4 GB"),
        "opt-1.3b":    ("1.3B",   "2.7 GB"),
        "pythia-70m":  ("70M",    "0.3 GB"),
        "pythia-160m": ("160M",   "0.6 GB"),
        "tinyllama":   ("1.1B",   "2.2 GB"),
        "llama-2-7b":  ("7B",     "14 GB"),
        "mistral-7b":  ("7B",     "14 GB"),
        "llama-2-13b": ("13B",    "26 GB"),
    }
    for key, (params, mem) in table.items():
        if key in m:
            return f" Parameters:  {params}  |   Estimated RAM/VRAM:  {mem} "
    return " Parameters:  unknown  |   Estimated RAM/VRAM:  unknown "

LORA_TARGET_MAP = {
    "gpt2":      ["c_attn"],
    "gpt_neo":   ["q_proj", "v_proj"],
    "opt":       ["q_proj", "v_proj"],
    "llama":     ["q_proj", "v_proj"],
    "mistral":   ["q_proj", "v_proj"],
    "pythia":    ["query_key_value"],
    "falcon":    ["query_key_value"],
    "tinyllama": ["q_proj", "v_proj"],
    "default":   ["q_proj", "v_proj"],
}
def get_lora_targets(model_name: str) -> list:
    m = model_name.lower().strip()
    for key, targets in LORA_TARGET_MAP.items():
        if key in m:
            return targets
    return LORA_TARGET_MAP["default"]

def preprocess_function(examples, tokenizer, max_length: int, task_type: str):
    if task_type == "instruction":
        texts = [
            f"### Instruction:\n{inst}\n\n### Response:\n{out}"
            for inst, out in zip(examples["instruction"], examples["output"])
        ]
    else:
        texts = examples["text"]
    tokenized = tokenizer(
        texts,
        truncation=True,
        padding="max_length",
        max_length=max_length,
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

# PATCH for UI widgets and column mapping everywhere:
# All references to keys/columns/models should use .strip() and never contain trailing spaces.

# PATCH summary:
# 1. All keys/names/columns are normalized (no trailing spaces)
# 2. DataFrame and Dataset columns are stripped on load/mapping
# 3. Model IDs lowercased and stripped before lookups
# 4. UI column mapping and event handlers also normalized

# All other logic remains unchanged.